{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ibrahimalish/ibrahimalish/blob/main/Untitled3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "# Function to clean text\n",
        "def clean_text(text):\n",
        "    # Convert text to lowercase\n",
        "    text = text.lower()\n",
        "    # Remove punctuation\n",
        "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
        "    return text\n",
        "data = pd.read_csv('sample.csv')\n",
        "# Select a smaller subset of data for testing\n",
        "data_subset = data[['ARTICLE_ID', 'SECTION_TEXT']].head(100)\n",
        "\n",
        "#PRE-PROCESSING\n",
        "# Drop rows with missing values\n",
        "data_subset.dropna(inplace=True)\n",
        "\n",
        "# print(data_subset)\n",
        "\n"
      ],
      "metadata": {
        "id": "J9VHhgHN78t9"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def tokenize_text(text):\n",
        "    return text.strip().split()\n",
        "\n",
        "# Tokenize SECTION_TEXT for each row\n",
        "# for index, row in data_subset.iterrows():\n",
        "#     tokenized_text = tokenize_text(row['SECTION_TEXT'])\n",
        "#     # Do something with the tokenized text, such as printing it\n",
        "#     print(f\"Article ID: {row['ARTICLE_ID']}, Tokenized Text: {tokenized_text}\")\n"
      ],
      "metadata": {
        "id": "gXnPiHzSF2gm"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_ids = {}\n",
        "next_word_id = 0\n",
        "\n",
        "# Group the data by ARTICLE_ID\n",
        "grouped_data = data_subset.groupby('ARTICLE_ID')\n",
        "\n",
        "# Iterate over each group (article)\n",
        "for _, group in grouped_data:\n",
        "    # Iterate over each section_text in the group\n",
        "    for section_text in group['SECTION_TEXT']:\n",
        "        # Tokenize the section_text\n",
        "        tokens = section_text.strip().split()\n",
        "        # Assign a unique ID to each word\n",
        "        for word in tokens:\n",
        "            if word not in word_ids:\n",
        "                word_ids[word] = next_word_id\n",
        "                next_word_id += 1\n",
        "\n",
        "# Display the word IDs\n",
        "# for word, word_id in word_ids.items():\n",
        "\n",
        "#     print(f\"Word: {word}, Word ID: {word_id}\")"
      ],
      "metadata": {
        "id": "mdmA7ZWQKxbK"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "article_word_frequencies = {}\n",
        "\n",
        "# Group the data by ARTICLE_ID\n",
        "grouped_data = data_subset.groupby('ARTICLE_ID')\n",
        "\n",
        "# Iterate over each group (article)\n",
        "for article_id, group in grouped_data:\n",
        "    # Initialize a dictionary to store word frequencies for the current article ID\n",
        "    word_frequencies = {}\n",
        "\n",
        "    # Iterate over each section_text in the group\n",
        "    for section_text in group['SECTION_TEXT']:\n",
        "        # Tokenize the section_text\n",
        "        tokens = section_text.strip().split()\n",
        "        # Calculate word frequencies\n",
        "        for word in tokens:\n",
        "            word_id = word_ids.setdefault(word, next_word_id)\n",
        "            if word_id == next_word_id:\n",
        "                next_word_id += 1\n",
        "            word_frequencies[word_id] = word_frequencies.get(word_id, 0) + 1\n",
        "\n",
        "    # Store word frequencies for the current article ID\n",
        "    article_word_frequencies[article_id] = word_frequencies\n",
        "\n",
        "# Display the article_word_frequencies dictionary\n",
        "\n",
        "# for article_id, word_frequencies in article_word_frequencies.items():\n",
        "\n",
        "#      print(f\"Article ID: {article_id}, Word Frequencies: {word_frequencies}\")"
      ],
      "metadata": {
        "id": "XEiyKjonNH81"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#idf_values = {0: 1, 1: 1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1, 13: 1, 14: 1, 15: 1, 16: 1, 17: 1, 18: 1, 19: 1, 20: 1, 21: 2, 22: 2, 23: 1, 24: 3, 25: 1, 26: 1, 27: 2, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1, 34: 1}\n",
        "\n",
        "# Calculate TF/IDF weights\n",
        "tfidf_weights = {}\n",
        "for article_id, word_frequencies in article_word_frequencies.items():\n",
        "    tfidf_weights[article_id] = {}\n",
        "    for word_id, frequency in word_frequencies.items():\n",
        "        tfidf_weights[article_id][word_id] = frequency * idf_values.get(word_id, 0)\n",
        "\n",
        "# Display the TF/IDF weights\n",
        "\n",
        "\n",
        "# for article_id, weights in tfidf_weights.items():\n",
        "#         print(f\"Article ID: {article_id}\")\n",
        "#         for word_id, weight in weights.items():\n",
        "#             print(f\"    Word ID: {word_id}, TF/IDF Weight: {weight}\")"
      ],
      "metadata": {
        "id": "dzDlPEsG2q99"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PHzutlXaSlxI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}